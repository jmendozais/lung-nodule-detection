{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MNIST\n"
     ]
    }
   ],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'Placeholder_21:0' shape=(?, 28, 28, 1) dtype=float32>, <tf.Tensor 'MaxPool_14:0' shape=(?, 14, 14, 32) dtype=float32>, <tf.Tensor 'MaxPool_15:0' shape=(?, 7, 7, 64) dtype=float32>]\n",
      "Training stage\n",
      "loss 5.71879720688  accuracy 0.119999997318  \n",
      "Testing stage\n",
      "loss 2.55711627007\n",
      "accuracy 0.17950001359\n",
      "0\n",
      "Tensor(\"MaxPool_14:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
      "Training stage\n",
      "loss 764.595581055  \n",
      "loss 82.1227416992  \n",
      "loss 37.47706604  \n",
      "loss 24.4473075867  \n",
      "loss 20.0887870789  \n",
      "Testing stage\n",
      "loss 17.7117424011\n",
      "1\n",
      "Tensor(\"MaxPool_15:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
      "Training stage\n",
      "loss 37217.7265625  \n",
      "loss 487.566833496  \n",
      "loss 317.579833984  \n",
      "loss 225.838027954  \n",
      "loss 191.764831543  \n",
      "Testing stage\n",
      "loss 162.297546387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ns_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))\\nu_loss = tf.reduce_mean(tf.reduce_sum(tf.square(x - x_reconstructed), axis=(1, 2)))\\nloss = s_loss + u_loss\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Utils \n",
    "\n",
    "class AttributeDict(dict):\n",
    "    __getattr__ = dict.__getitem__\n",
    "\n",
    "    def __setattr__(self, a, b):\n",
    "        self.__setitem__(a, b)\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def resize_images(X, height_factor, width_factor):\n",
    "    shape = X.get_shape()\n",
    "    original_shape = tuple([i.__int__() for i in shape])\n",
    "    \n",
    "    new_shape = tf.shape(X)[1:3]\n",
    "    new_shape *= tf.constant(np.array([height_factor, width_factor]).astype('int32'))\n",
    "    X = tf.image.resize_nearest_neighbor(X, new_shape)\n",
    "    X.set_shape((None, original_shape[1] * height_factor if original_shape[1] is not None else None,\n",
    "                original_shape[2] * width_factor if original_shape[2] is not None else None, None))\n",
    "    return X\n",
    "\n",
    "def up_sample_2x2(X):\n",
    "    return resize_images(X, 2, 2)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def create_network(input_shape, filters, filters_size, dense, dropout):\n",
    "    d = AttributeDict()\n",
    "    \n",
    "    d.x = tf.placeholder(tf.float32, shape=[None] + list(input_shape))\n",
    "    d.y = tf.placeholder(tf.float32, shape=[None, dense[-1]])\n",
    "    \n",
    "    d.input_shape = input_shape\n",
    "    d.do_tr = dropout\n",
    "    d.filters = filters\n",
    "    d.filters_size = filters_size\n",
    "    d.dense = dense\n",
    "\n",
    "    d.W = []\n",
    "    d.b = []\n",
    "    d.do = []\n",
    "    \n",
    "    d.h = [d.x]\n",
    "    d.mp_h = []\n",
    "    \n",
    "    L_conv = len(filters)\n",
    "    filters = [input_shape[-1]] + filters\n",
    "    \n",
    "    for i in range(L_conv):\n",
    "        # Convolution\n",
    "        wi = weight_variable((filters_size[i], filters_size[i], filters[i], filters[i+1]))\n",
    "        bi = bias_variable((filters[i+1],))\n",
    "        hi = tf.nn.relu(conv2d(d['h'][-1], wi) + bi)\n",
    "        \n",
    "        if dropout[i] != .0:\n",
    "            doi = tf.placeholder(tf.float32)\n",
    "            hi = tf.nn.dropout(hi, keep_prob=1.0 - doi)\n",
    "            d.do.append(doi)\n",
    "        else:\n",
    "            d.do.append(None)\n",
    "            \n",
    "        hi = max_pool_2x2(hi)\n",
    "        \n",
    "        d.W.append(wi)\n",
    "        d.b.append(bi)\n",
    "        d.h.append(hi)\n",
    "        \n",
    "        #d.mp_h.append(max_pool_2x2(hi))\n",
    "    \n",
    "    L_dense = len(dense)\n",
    "    conv_output_shape = input_shape[0] / (2**L_conv)\n",
    "    dense = [filters[-1] * (conv_output_shape ** 2),] + dense\n",
    "    \n",
    "    print d.h\n",
    "    for i in range(L_dense):\n",
    "        if i == 0:\n",
    "            hi = tf.reshape(d.h[-1], shape=(-1, dense[0]))\n",
    "        wi = weight_variable((dense[i], dense[i+1]))\n",
    "        bi = bias_variable((dense[i+1],))\n",
    "        hi = tf.nn.relu(tf.matmul(hi, wi) + bi)\n",
    "        \n",
    "        if dropout[L_conv + i] != .0:\n",
    "            doi = tf.placeholder(tf.float32)\n",
    "            hi = tf.nn.dropout(hi, keep_prob=1.0 - doi)\n",
    "            d.do.append(doi)\n",
    "        else:\n",
    "            d.do.append(None)\n",
    "        \n",
    "        d.W.append(wi)\n",
    "        d.b.append(bi)\n",
    "        d.h.append(hi)\n",
    "    return d\n",
    "\n",
    "def fit(sess, net, loss, data, metrics={}, iterations=500):\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    \n",
    "    metrics['loss'] = loss\n",
    "    to_fetch = [metrics[k] for k in metrics]\n",
    "    to_fetch_labels = [k for k in metrics]\n",
    "    \n",
    "    feed_dict = {} \n",
    "    for i in range(len(net.do_tr)):\n",
    "        if net.do[i] != None:\n",
    "            feed_dict[net.do[i]] = net.do_tr[i]\n",
    "    \n",
    "    print \"Training stage\"\n",
    "    for i in range(iterations):\n",
    "        batch = data.next_batch(50)  \n",
    "        feed_dict[net.x] = batch[0]\n",
    "        feed_dict[net.y] = batch[1]\n",
    "        if i%100 == 0:\n",
    "            fetched = sess.run(to_fetch, feed_dict=feed_dict)\n",
    "            for i in range(len(fetched)):\n",
    "                print \"{} {} \".format(to_fetch_labels[i], fetched[i]),\n",
    "            print ''\n",
    "                \n",
    "        train_step.run(feed_dict=feed_dict)\n",
    "        \n",
    "    print \"Testing stage\"\n",
    "    feed_dict = {net.x: data.X_test, net.y: data.Y_test}\n",
    "    for i in range(len(net.do)):\n",
    "        if net.do[i] != None:\n",
    "            feed_dict[net.do[i]] = .0\n",
    "    \n",
    "    fetched = sess.run(to_fetch, feed_dict=feed_dict)\n",
    "    \n",
    "    for i in range(len(fetched)):\n",
    "        print(\"{} {}\".format(to_fetch_labels[i], fetched[i]))\n",
    "    \n",
    "\n",
    "def fit_semisupervised(sess, net, loss, data, metrics={}):\n",
    "    L_conv = len(net.filters)\n",
    "    \n",
    "    net.d = AttributeDict()\n",
    "    net.d.W = [None for i in range(L_conv)]\n",
    "    net.d.b = [None for i in range(L_conv)]\n",
    "    net.d.h = [None for i in range(L_conv+1)]\n",
    "    \n",
    "    # Layerwise initialization\n",
    "    filters = [input_shape[-1]] + net.filters\n",
    "\n",
    "    for i in range(L_conv):\n",
    "        print i\n",
    "        net.d.h[i+1] = net.h[i+1]\n",
    "        print net.d.h[i+1]\n",
    "\n",
    "        hi = up_sample_2x2(net.d.h[i+1])\n",
    "        wi = weight_variable((net.filters_size[i], net.filters_size[i], filters[i+1], filters[i]))\n",
    "        bi = bias_variable((filters[i],))\n",
    "        if i > 0:\n",
    "            hi = tf.nn.relu(conv2d(hi, wi) + bi)\n",
    "        else:\n",
    "            hi = conv2d(hi, wi) + bi\n",
    "            \n",
    "        net.d.W[i] = wi\n",
    "        net.d.b[i] = bi\n",
    "        net.d.h[i] = hi\n",
    "        \n",
    "        for j in reversed(range(i)):\n",
    "            net.d.h[j] = up_sample_2x2(net.d.h[j+1])\n",
    "            if j > 0:\n",
    "                net.d.h[j] = tf.nn.relu(conv2d(net.d.h[j], net.d.W[j]) + net.d.b[j])\n",
    "            else:\n",
    "                net.d.h[j] = conv2d(net.d.h[j], net.d.W[j]) + net.d.b[j]\n",
    "        \n",
    "        # fit\n",
    "        layerwise_loss = tf.reduce_mean(tf.reduce_sum(tf.square(net.d.h[0] - net.x), axis=(1, 2)))\n",
    "        fit(sess, net, layerwise_loss, data, iterations=500)\n",
    "    \n",
    "    # Decoding pathway training\n",
    "    \n",
    "    # Enconding/decoding finetuning\n",
    "    \n",
    "    return net\n",
    "\n",
    "# 2 Difference between run on variables vs run on session\n",
    "# 3 Fit swwae\n",
    "\n",
    "data = SemiSupervisedData(50000)\n",
    "input_shape = (28, 28, 1)\n",
    "filters = [32, 64]\n",
    "filters_size = [5, 5]\n",
    "dense = [1024, 10]\n",
    "dropout = [.0, .0, .5, .0]\n",
    "\n",
    "sess = tf.InteractiveSession() \n",
    "\n",
    "architecture = {'input_shape':(28, 28, 1), 'filters':[32, 64], 'filters_size':[5, 5], \n",
    "                'dense':[1024, 10], 'dropout':[.0, .0, .5, .0]}\n",
    "\n",
    "net = create_network(**architecture)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net.h[-1], net.y))\n",
    "correct_prediction = tf.equal(tf.argmax(net.h[-1],1), tf.argmax(net.y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "fit(sess, net, loss, data, metrics={'accuracy':accuracy}, iterations=1)\n",
    "\n",
    "fit_semisupervised(sess, net, loss, data)\n",
    "\n",
    "sess.close()\n",
    "\n",
    "\n",
    "'''\n",
    "aug_net = augment_network(net, **architecture)\n",
    "loss = ...\n",
    "fit(sess, aug_net, loss)\n",
    "'''\n",
    "\n",
    "# Fit \n",
    "''' \n",
    "How to implement s_loss + u_loss?\n",
    "\n",
    "Papers suchs as SWWAE[1] mentions that u and s objetives should be calculated in the same phase. I assumed\n",
    "that the labeled data used on a SGD iteration should be used for class/rec phase, and unlabeled data should \n",
    "used just for rec phase. However, ladder networks implementation[2] shows that labeled could be used for\n",
    "classification only and unlabeled stream could include labeled and unlabeled data, computing only the \n",
    "reconstruction loss. So we have two options:\n",
    "\n",
    "1) s_loss(labeled) + u_loss(unlabeled + labeled)\n",
    "2) s_loss(labeled) + u_lsss(unlabeled*) # unlabeled* iterated over the whole dataset (labeled+unlabeled)\n",
    "\n",
    "# The diference between both is that in 2) u_loss will not necessarily consider the same labeled images used\n",
    "to compute s_loss\n",
    "\n",
    "[1] https://arxiv.org/pdf/1506.02351v8.pdf\n",
    "[2] https://github.com/rinuboney/ladder/blob/master/input_data.py\n",
    "'''\n",
    "'''\n",
    "for i in range(L):\n",
    "    diff_i = augmented_network['h'][i] - augmented_network['ih'][i]\n",
    "    loss_i = tf.reduce_mean(tf.reduce_sum(tf.square(diff_i), axis=(1, 2)))\n",
    "    fit(sess, data, loss_i)\n",
    "'''\n",
    "'''\n",
    "s_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))\n",
    "u_loss = tf.reduce_mean(tf.reduce_sum(tf.square(x - x_reconstructed), axis=(1, 2)))\n",
    "loss = s_loss + u_loss\n",
    "'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
